---
layout: post
title:  "[리뷰] (DCT-Net) DCT-Net: Domain-Calibrated Translation for Portrait Stylization"
date:   2023-02-21 22:16:00 +0900
categories: [GAN, Few-Shot Learning]
tags: [GAN, Image Translation, Few-Shot Learning]
use_math: true
---

{: .note .warning}
공부하면서 정리한 내용이므로 부정확한 내용이 포함되어 있을 수 있음.

Title: [DCT-Net: Domain-Calibrated Translation for Portrait Stylization]
<br/>
Authors: Y. Men, Y. Yao, M. Cui, Z. Lian, and X. Xe
<br/>
Submitted on 6 Jul 2022
<br/>
Project Page: [Github]

## Abstract

* ???

![Fig1](/assets/dct-net/fig1.png)

그림 1: 제한된 스타일 예시가 주어졌을 때, 본 기법은 해당 스타일로 예술적인 초상화를 합성할 수 있고, 콘텐츠 (예: 아이덴티티 및 악세서리) 보존에 뛰어나고, heavy occlusion, 화장, 흔하지 않은 자세의 복잡한 얼굴을 처리함. 본 기법은 또한 트레이닝 샘플들에 대한 머리 관찰만으로 full-body 변환을 가능하게 함. 소스 크레딧: 머리 인풋 및 full-body 인풋 [Pexels 웹사이트]

![Fig2](/assets/dct-net/fig2.png)

그림 2: 도메인 보정 변환(domain-calibrated translation)의 묘사. 다양한 소스 분포로부터 few-shot 예시들로 형성된 편향된 타깃 분포로의 대응을 학습하는 것은 어려움. 먼저 소스 샘플들을 적응시켜 콘텐츠 특징점들에서 타깃 도메인 $$\mathcal{D}_t$$의 분포를 보정하고, 그 다음 $$\mathcal{D}_t$$를 기하학적 차원에서 확장함. 보정된 분포로부터 샘플링된 예시들로, 진보된 능력, 일반성, 규모 확장성으로 세분화된 텍스처 변환을 학습하는 것이 더 쉬워짐.

## Introduction

* 본 논문의 핵심 통찰은 세 가지임.
    * "보정(calibration) 우선, 변환(translation) 나중" 전략은 안정적인 교차 도메인 변환을 쉽게 학습하고 높은 충실도의 결과를 도출할 수 있도록 함.
    * 균형 잡힌 소스 분포(source distribution)는 타깃 도메인(target domain)의 편향된 콘텐츠 분포를 보정하기 위해 이전 작업으로 사용될 수 있음.
    * 기하학적 확장을 통해 공간적 semantic 제약을 해제하면 더 유연하고 광범위한 추론(inference)이 가능해짐.
* 이를 위해, 학습된 소스 생성기를 타깃 도메인에 적응시켜(즉, 소스에서 강력한 콘텐츠를 먼저 차용) 타깃 분포의 콘텐츠 특징점들을 먼저 보정하는 "도메인 보정 변환(domain-calibrated translation"을 위한 간단하면서 효과적인 솔루션을 제안함.
* 그 다음, 기하학적 확장 모듈(geometry expansion module)을 사용하여 도메인 특징점들이 더욱 강화됨.
* 이러한 보정된 분포를 통해 non-local correlation을 설명하는 다양한 예를 적절한 수량만큼 생성할 수 있으며, 로컬 동작이 강한 네트워크, U-net을 트레이닝하여 교차 도메인 변환을 수행함.
* 이 설계는 본 기법이 로컬에 초점을 맞춘 변환으로 증강된 글로벌 구조를 학습할 수 있게 하고 전반적인 개선을 가져옴.
* 우리의 트레이닝된 모델은 세부 콘텐츠(아이덴티티, 악세서리, 배경 등)를 보존할 뿐 아니라, 복잡한 장면(heavy occlusion과 흔하지 않은 자세 등)을 다루는 데에도 뛰어남. 또한 full-body 이미지 변환 같은 도메인 변환이 가능하도록 하여, 변환의 일반화 능력을 크게 증가시킴.
* 이 새로운 task는 오직 raw head collection으로만 트레이닝 됐을 때 적응적 변형(deformation)을 요구함.
* 알려진 바로는, 이것은 "도메인 보정 변환"의 구조를 제안하고 상술한 관점들에서 우월성을 보인 최초의 접근법임.

## Method Description

### Overview

* 타깃 스타일 예시들의 작은 셋이 주어졌을 때, 목표는 소스 도메인 $$X_s$$에서 타깃 도메인 $$X_t$$으로 이미지를 매핑하는 함수 $$M_{s \rightarrow t}$$를 학습하는 것임.
* 아웃풋 이미지 $$x_g$$는 타깃 예시 $$x_t$$의 유사한 텍스처 스타일로 표현되면서, 소스 이미지 $$x_s$$의 콘텐츠 세부 내용(예: 구조 및 아이덴티티)을 보존해야 함.

![Fig3](/assets/dct-net/fig3.png)

그림 3: 제안된 프레임워크의 overview로, 콘텐츠 보정 네트워크 (CCN), 기하학적 확장 모듈 (GEM), 텍스처 변환 네트워크(TTN)로 구성됨. CCN은 실제 얼굴 생성기 $$G_s$$로부터 콘텐츠를 미리 차용하고 타깃 도메인에 적응시켜, 타깃 도메인의 콘텐츠 분포를 보정하고 콘텐츠 대칭 특징점들을 취득함. GEM은 두 도메인의 기하학적 분포를 확장시키고 공간적인 제약을 완화하고 기하학적 대칭을 향상시킴. 보정된 도메인으로, TTN을 채택하여 다수 표현과 로컬 지각적 제약으로 교차 도메인 변환을 학습함. CCN과 TTN은 독립적으로 트레이닝됨. 트레이닝 후, TTN만이 최종 추론에 사용됨.

* 제안된 프레임워크의 overview는 그림 3에 나와 있음. 다음 세 가지 모듈, 콘텐츠 보정 네트워크 (CCN), 기하학적 확장 모듈 (GEM), 텍스처 변환 네트워크(TTN)로 순차적인 파이프라인을 구축함.
    * CCN: 사전 트레이닝된 소스 생성기 $$G_s$$로부터 타깃 스타일을 전이 학습(transfer learning)으로 타깃 분포를 콘텐츠 차원에서 보정함.
    * GEM: 소스와 타깃 분포의 기하학 차원을 더욱 확장시키고, 나중의 변환을 위한 다양한 스케일과 회전으로 기하학 대칭 특징점들을 제공함.
    * TTN: 캘리브레이션 된 분포로부터 샘플링된 데이터로, 다수 표현 제약과 로컬 지각 loss로 교차 도메인 대응을 학습함.
* CCN와 TTN은 독립적으로 트레이닝되고, TTN만이 최종 추론에 사용됨.

### Content calibration network

* 이 모듈에서는, 충분한 예에서 학습한 네트워크 파라미터를 전송하여 몇 가지 타깃 샘플의 편향된 분포를 보정함.
* 이미지 변환에 대해 StyleGAN2[Karas et al. 2020]를 inversion 기법과 결합한 이전 연구들([Pinkney and Adler 2020], [Richardson et al. 2021], [Song et al. 2021])과는 다르게, 사전 트레이닝된 StyleGAN2로부터 강력한 사전 기능을 활용하여 향상된 콘텐츠 대칭으로 타깃 도메인을 재구성함.
* 실제 얼굴(예: FFHQ 데이터셋)에 대해 트레이닝된 StyleGAN2 기반 모델 $$G_s$$에서 시작하여, $$G_s$$의 복사본인 $$G_t$$가 초기화 가중치(initialization weights)로 사용되며 $$G_t$$를 적용하여 타깃 도메인 $$X_t$$에서 이미지를 생성함.
* CCN의 트레이닝 단계에서, $$\hat{x}_t \in X_t$$와 기존 얼굴 인식 모델 $$R_{id}$$([Deng et al. 2019])이 $$\hat{x}_t$$와 $$\hat{x}_s$$ 사이의 인물 아이덴티티를 보존하도록 하기 위해 $$G_t$$와 판별기 $$D_t$$를 finetuning 함.
* CCN의 추론 단계 동안, $$G_s$$의 처음 $$k$$개의 레이어를 $$G_t$$의 해당 레이어들과 혼합하는데, 이는 원래 소스 도메인의 더 많은 콘텐츠를 보존하는 데 효과적인 것으로 입증됨([Pinkney and Adler 2020]).
* 이러한 방식으로, $$\hat{x}_s$$와 $$\hat{x}_t$$와 같은 소스 및 타깃 도메인에서 상대적으로 콘텐츠 대칭 이미지를 생성할 수 있음. 흐름도는 그림 4에 표시됨.

![Fig4](/assets/dct-net/fig4.png)

그림 4: CCN의 흐름도.

* 직접 $$z$$ 공간에서 샘플링하고 콘텐츠 대칭 방식(즉, 두 가지 디코딩 경로에 대한 같은 $$z$$)으로 소스와 타깃 도메인 $$(\hat{X}_s, \hat{X}_t)$$을 재구성하는 것이 중요함.
* Inversion 임베딩이 필요하고 축적된 오차를 초래하는 실제 얼굴들은 사용하지 않음.
* 실세계 사진의 충분한 데이터로 인해, 분포 $$\mathcal{D}(\hat{X}_s)$$는 실제 분포 $$\mathcal{D}(X_s)$$를 극히 간략화할 수 있음. 그래서, $$\mathcal{D}(\hat{X}_t)$$는 $$\mathcal{D}(X_s)$$와 상대적으로 대칭이고, 나중의 단계에서 소스와 타깃 사이의 교차 도메인 대응을 학습하기 쉽도록 함.
* 반대로, 이전 기법들([Pinkney and Adler 2020], [Richardson et al. 2021], [Song et al. 2021])은 전형적으로 StyleGAN2를 inversion 기법들과 결합하며([Abdal et al. 2020], [Tov et al. 2021]), 소스 이미지들을 $$z$$ 공간이나 StyleGAN2의 $$\mathcal{W} / \mathcal{W}+$$ 공간에 매핑하고 이 무조건부 생성기를 활용하여 대응하는 결과를 합성함.
* 따라서, 임의의 초상화(예: 도메인 외 이미지)가 저차원의 $$z$$ 공간이나 스타일이 분리된 $$\mathcal{W} / \mathcal{W}+$$ 공간에 임베딩될 수 있음을 보장하는 것은 어려운데, 이는 [Roich et al. 2021], [Tov et al. 2021]에서 설명된 것처럼 "왜곡-편집성 trade-off" 때문임.
* 이 inversion 프로세스는 이미지 변환 task에 대해 추가 아이덴티티와 빠진 구조 세부 내용을 이끌어냄.

### Geometry expansion module

* 이전 모듈은 소스 분포를 타깃 분포를 보정하기 위한 ground-truth 분포로 사용함.
* 그러나, 소스 도메인 (FFHQ) 내 모든 이미지들은 표준 얼굴 위치로 align 돼 있고, 이는 네트워크가 합성을 위한 위치적 의미에 심히 의존하게 되고 더 나아가 실세계 이미지를 처리하기 위한 네트워크의 성능을 제한함.
* 이런 제약을 풀고 **Inference**에 기술된 전체 이미지 추론을 지원하기 위해, 기하학 변환 $$T_{Geo}$$를 소스 샘플들 $$\hat{x}_s / x_s$$과 타깃 샘플들 $$\hat{x}_t$$ 둘 다에 적용하여, 기하학 확장 샘플들 $$\tilde{x}_s$$와 $$\tilde{x}_t$$를 생산함.
* $$T_{Geo}$$는 랜덤 스케일 비율 $$\mu \in [0.8, 1.2]$$와 랜덤 회전 각도 $$\gamma \in [-\frac{\pi}{2}, \frac{\pi}{2}]$$로 수행됨.

### Texture translation network

* TTN은 비지도 방식으로 보정된 도메인들 $$(\tilde{X}_s, \tilde{X}_t)$$ 사이에서 교차 도메인 대응 학습을 목표로 함.
* 첫 번째 모듈은 샘플링된 노이즈 $$z$$로 정렬된 짝들을 대략적으로 생산할 수 있으나, 글로벌 매핑의 특성으로 인해 콘텐츠의 세부 내용을 보존하지 못하고 또한 추가적인 inversion 오차로 임의의 실제 얼굴을 처리할 수 없음.
* 재구성된 두 도메인에서 텍스처 정보가 충분하지만 그것들 사이의 텍스처 매핑이 부정확한 것을 고려하여, 매핑 네트워크 $$\mathcal{M}_{s \rightarrow t}$$에 U-net 구조를 도입하여([Ronneberger et al. 2015]) 글로벌 도메인 매핑을 로컬 텍스처 변형으로 전환하고, 픽셀 레벨에서 세분화된 텍스처 변환을 학습함.
<br/><br/>
* 소스 사진의 충분한 데이터의 활용으로, 재구성된 소스 분포는 원본 소스 분포를 극도로 간략화할 수 있음($$D(X_s) \approx D(\hat{X}_s)$$). 그래서, 실제 소스들(기하학 확장 전에)과 보정된 타깃 샘플들(콘텐츠와 기하학 보정 후)을 대칭 변환을 위해 사용함. 이 프로세스에서, 대칭 특징점들은 이미지 레벨에서 도메인 레벨로 전환됨.
* 제안된 TTN이 짝이 없는 (unpaired) 이미지로 비지도적 방식으로 트레이닝됨. 실제 이미지들을 인풋으로 사용할 때에도, 상응하는 스타일화된 샘플들을 생산하는 데 inversion 기법이 필요하지 않음.
* 스타일 이미지 $$\tilde{x}_t \in \tilde{X}_t$$는 랜덤으로 샘플링되고 ground truth와는 다른 스타일 표현을 제공하기 위해서만 사용되며, 이는 로컬 최적화로부터 벗어나기 위함임.
* 단순히 본 기법을 간결하고 직관적으로 설명하기 위해 그림 3의 모든 모듈에 대해 동일한 샘플을 사용함.

#### Multi-representation constraints

* [Wang and Yu 2020]에서의 표현 분리 방식에 착안하여, 텍스처와 표면 분리를 통해 $$\tilde{x}_t$$와 $$x_g$$로부터 스타일 표현 $$\mathcal{F}_{sty}$$를 추출하고, 판별기 $$D_s$$를 사용하여 $$\mathcal{M}_{s \rightarrow t}$$가 유사한 스타일의 $$\tilde{x}_t$$로 $$x_g$$를 합성하도록 가이드함.
* 스타일 loss $$\mathcal{L}_{sty}$$는 실제 스타일화된 이미지와 생성된 이미지의 스타일 표현 분포들 사이의 거리(distance)를 페널티로 하여 계산됨.
<br/><br/>
![Eq1](/assets/dct-net/eq1.png)
<br/><br/>
* 사전 트레이닝된 VGG16 네트워크 ([Simonyan and Zisserman 2014])는 소스 이미지들 $$\tilde{x}_s$$와 생성된 이미지들 $$x_g$$로부터 콘텐츠 표현 $$\mathcal{F}_{con}$$을 추출하여 콘텐츠 일관성을 보장하는 데 사용함.
* 콘텐츠 loss $$\mathcal{L}_{con}$$은 VGG 특징점 공간에서 $$x_g$$와 $$\tilde{x}_s$$ 사이의 L1 거리로 공식화됨.
<br/><br/>
![Eq2](/assets/dct-net/eq2.png)
<br/><br/>

#### Facial perception constraint

* 네트워크가 (단순화된 입과 크고 섬세한 눈과 같은) 구조 변형이 과장된 스타일화된 초상화를 생산하도록 더욱 유도하기 위해, 합성 프로세스를 가이드하는 보조 표현 회귀자 (regressor) $$\mathcal{R}_{exp}$$를 도입함.
    * 본질적으로 얼굴 구성 요소(예: 입과 눈)의 영역에 더 큰 어텐션을 주는 $$\mathcal{R}_{exp}$$를 통해 합성 이미지의 얼굴 표현에 제약을 줌으로써 로컬 구조 변형을 자극함.
* 구체적으로, $$\mathcal{R}_{exp}$$는 형상 추출기 $$\mathcal{E}_{f}$$ 위에 $$n$$개의 회귀 헤드(regression head)로 구성되며, 여기서 $$n$$은 표현 파라미터의 수를 나타냄.
* $$\mathcal{E}_{f}$$와 $$D_s$$는 모두 PatchGAN 구조([Isola et al. 2017])를 따름.
* 더 빠른 트레이닝 과정을 달성하기 위해, 학습된 회귀자를 직접 적용하여 생성된 이미지들 $$x_g$$의 표현 스코어를 추정함.
* 얼굴 perception loss는 아래와 같이 계산됨.
<br/><br/>
![Eq3](/assets/dct-net/eq3.png)
<br/><br/>
여기서 $$\pmb{\alpha} = \pmb{\alpha}_1, \cdots, \pmb{\alpha}_n$$는 소스 이미지 $$\tilde{x}_s$$로부터 추출된 표현 파라미터들임. $$n = 3$$으로 놓고 $$\pmb{\alpha}_i \in [0, 1]$$은 각각 왼쪽 눈, 오른쪽 눈, 입의 여는 각도로 정의함.
* $$\tilde{x}_s$$로부터 추출된 얼굴 점들 $$p$$로, $$\pmb{\alpha}_i$$는 특정 얼굴 성분의 bounding box의 높이 대 너비 비율을 계산하여 쉽게 얻을 수 있음.

#### Training

* 보정된 소스와 타깃 도메인으로부터 $$\tilde{x}_s$$와 $$\tilde{x}_t$$가 주어졌을 때, 텍스처 변환 모델은 스타일 항, 콘텐츠 항, 얼굴 perception 항, 총 variation 항으로 구성된 전체 loss 함수로 트레이닝됨.
<br/><br/>
![Eq4](/assets/dct-net/eq4.png)
<br/><br/>
여기서 $$\lambda$$는 각 대응하는 loss의 가중치를 나타냄. 총 variation loss $$\mathcal{L}_{tv}$$는 생성된 이미지 $$x_g$$를 부드럽게 하는 데 사용되는데, 다음과 같이 계산될 수 있음.
<br/><br/>
![Eq5](/assets/dct-net/eq5.png)
<br/><br/>
여기서 $$u$$와 $$v$$는 각각 수평과 수직 방향을 나타냄.

![Fig6](/assets/dct-net/fig6.png)

그림 6: 전체 이미지 변환의 파이프라인. 다른 기존의 접근법과 같이 복잡한 구조를 활용하기보다는, 한 번의 평가로 명쾌한 단일 네트워크에서 목표를 달성함.

### Inference

* 정렬된 얼굴 스타일화로 제한된 이전 작품([Kim et al. 2020], [Song et al. 2021])과 달리, 본 모델은 회전에서 여러 얼굴을 포함하는 임의의 초상화 이미지에 대해 전체 이미지 렌더링을 가능하게 함.
* 위 목표를 달성하기 위한 일반적인 관행은 얼굴과 배경을 독립적으로 처리하는 것임.
    * 그림 6과 같이, 먼저 입력 이미지에서 정렬된 얼굴을 추출하고 모든 얼굴을 하나씩 스타일화함.
    * 그 다음 배경 이미지는 일부 특수 알고리즘으로 렌더링되고 양식화된 얼굴과 병합되어 최종 결과를 얻음.
* 이런 복잡한 파이프라인을 사용하는 대신, 본 기법의 TTN이 one-pass 평가에서 전체 이미지에서 스타일화된 결과를 집적 나타낼 수 있음이 발견됨.
* 도메인 보정 이미지들을 사용하여, 네트워크는 트레이닝 중에 전체 텍스처 콘텐츠를 보기 때문에, 얼굴 생김새뿐만 아니라 배경의 맥락 정보를 암시적으로 인코딩함.
* GEM과 결합되어 맨 얼굴 처리에 대해 스케일과 회전이 불변함. GEM에서 스케일 비율의 범위가 채택되므로, 인풋 이미지는 모두 만족스럽게 처리될 수 있는 스케일로 리사이즈됨.
* 실험적으로 해상도가 $$2K \times 2K$$ 미만인 이미지가 합성된 이미지에서 블러가 없이 잘 처리될 수 있음이 발견됨.

## Conclusion

* 머리 스타일화 작업에 대한 능력, 일반성 및 확장성을 향상시킬 뿐만 아니라 명쾌한 방식으로 효과적인 full-body 이미지 변환을 달성하는 스타일화된 초상화 생성을 위한 새로운 프레임워크인 DCT-Net을 제안함.
* 핵심 아이디어는 먼저 편향된 타깃 도메인을 보정하고 나중에 세분화된 변환을 학습하는 것임.
* 구체적으로는, 다양한 텍스처를 위해 콘텐츠 보정 네트워크(CCN)를 도입하고 공간적 제약을 해소하기 위해 기하학 확장 모듈(GEM)을 설계함. 이 두 모듈에 의해 생산된 보정된 샘플을 통해, 본 텍스처 변환 네트워크는 섬세하게 설계된 loss로 교차 도메인 대응을 쉽게 학습함.
* 실험 결과는 본 기법의 우수성과 효과를 입증함. 또한 도메인 보정 변환 솔루션이 편향된 타깃 분포를 가진 I2I 변환 task에 대한 향후 조사에 영감을 줄 수 있을 것임.

[DCT-Net: Domain-Calibrated Translation for Portrait Stylization]: https://arxiv.org/abs/2207.02426
[Github]: https://github.com/menyifang/DCT-Net
[Pexels 웹사이트]: https://www.pexels.com/
[Pinkney and Adler 2020]: https://arxiv.org/abs/2010.05334
[Richardson et al. 2021]: https://openaccess.thecvf.com/content/CVPR2021/html/Richardson_Encoding_in_Style_A_StyleGAN_Encoder_for_Image-to-Image_Translation_CVPR_2021_paper.html
[Song et al. 2021]: https://dl.acm.org/doi/abs/10.1145/3450626.3459771
[Karras et al. 2020]: https://openaccess.thecvf.com/content_CVPR_2020/html/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.html
[Deng et al. 2019]: https://openaccess.thecvf.com/content_CVPR_2019/html/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.html
[Abdal et al. 2020]: https://openaccess.thecvf.com/content_CVPR_2020/html/Abdal_Image2StyleGAN_How_to_Edit_the_Embedded_Images_CVPR_2020_paper.html
[Tov et al. 2021]: https://dl.acm.org/doi/10.1145/3450626.3459838
[Roich et al. 2021]: https://arxiv.org/abs/2106.05744
[Ronneberger et al. 2015]: https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28
[Wang and Yu 2020]: https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Learning_to_Cartoonize_Using_White-Box_Cartoon_Representations_CVPR_2020_paper.html
[Simonyan and Zisserman 2014]: https://arxiv.org/abs/1409.1556
[Isola et al. 2017]: https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html
[Kim et al. 2020]: https://iclr.cc/virtual_2020/poster_BJlZ5ySKPH.html