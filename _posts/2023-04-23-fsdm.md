---
layout: post
title:  "[리뷰] (FSDM) Few-shot Image Generation with Diffusion Models"
date:   2023-04-23 18:50:00 +0900
categories: [Diffusion Model]
tags: [Diffusion Model, Image Generation]
use_math: true
---

{: .note .warning}
공부하면서 정리한 내용이므로 부정확한 내용이 포함되어 있을 수 있음.

Title: [Few-Shot Diffusion Models]
<br/>
Authors: G. Giannone, D. Nielsen, and O. Winther
<br/>
Submitted on 30 May 2022
<br/>
Project Page: [Github]

## Abstract

* 디노이징 확산 확률 모델(DDPM)은 샘플 생성 품질과 트레이닝 안정성이 뛰어난 강력한 계층적 잠재 (latent) 변수 (variable) 모델임. 이러한 속성은 파라미터가 없는 확산 기반 추론 (inference) 절차뿐만 아니라 생성 계층의 파라미터 공유에 기인할 수 있음.
* 본 논문에서는 조건부 (conditional) DDPM을 활용한 few-shot 생성 프레임워크인 Few-Shot 확산 모델(FSDM)을 제안함.
    * FSDM은 set 기반 비전 트랜스포머(ViT)를 사용하여 이미지 패치 정보를 결합하여 주어진 클래스로부터의 소형의 이미지셋에서 조건부 생성 프로세스를 적용하도록 트레이닝됨.
    * 테스트 시 모델은 해당 클래스에서 5개의 샘플만 조건으로 이전에 보이지 않았던 클래스에서 샘플을 생성할 수 있음.
* 우리는 FSDM이 few-shot 생성을 수행하고 새로운 데이터셋으로 전환할 수 있음을 경험적으로 보여줌.
* 우리는 few-shot 학습을 위한 복잡한 비전 데이터셋에서 우리 방법의 변형을 벤치마킹하고 조건부 및 조건부 DDPM baseline과 비교함. 또한 패치 기반 입력 셋 정보에서 모델을 조건화하면 트레이닝 수렴이 어떻게 향상되는지 보여줌.

<div id="Fig1"></div>
![Fig1](/assets/fsdm/fig1.png)

그림 1. Few-Shot 확산 모델을 사용한 CIFAR100에서의 셋 (좌) 및 조건부 샘플 (우). FSDM은 소량의 현실적인 예시들로부터 콘텐츠 정보를 추출하고 다양한 조건부 분포로부터 풍부하고 복잡한 샘플들을 생성할 수 있음. 부록 [그림 6](#Fig6)에서 더 많은 샘플들 확인.

<div id="Sec1"></div>
## Introduction

* **Contributions**
    * DDPM 프레임워크에서 이미지의 현실적인 셋을 위한 few-shot 생성을 수행할 새로운 프레임워크
    * Learnable Attentive Conditioning (LAC), 즉 입력 셋이 패치 모음으로 처리되고 샘플 수준과 셋 수준의 변수 사이의 어텐션을 통한 DDPM을 조건화하기 위해 사용되는 조건화 메커니즘
    * 본 모델이 상대적으로 무조건적이고 조건부 DDPM 기반의 baseline에 비해서 조건부 및 few-shot 생성에 대해 트레이닝의 속도를 올리고, 샘플의 퀄리티와 다양성을 증가시키고, 전환을 개선시킨다는 실험적 증거

<div id="Sec3"></div>
## Few-Shot Diffusion Models

* 본 논문에서, 목표는 새로운 생성 task에 빠르게 적응하는 것을 학습하는 것임. 즉, 새 task로부터 이전에 보이지 않은 샘플들을 포함한 셋 $$\mathrm{X}$$으로 조건화된 few-shot 생성을 수행하고자 함.
* 이 문제를 확산 모델(diffusion model)을 사용하여 접근함. 확산 모델 $$p_\theta (\mathrm{x} \vert \mathrm{X})$$을 셋 $$\mathrm{X}$$으로 조건화하여 학습함. 본 접근 방식을 Few-Shot 확산 모델(FSDM)이라 칭함.
* 본 모델은 두 메인 파트로 나눌 수 있음.
    * 셋 $$\mathrm{X}$$의 콘텍스트 표현 $$\pmb{\mathrm{c}} = h_\phi(\mathrm{X})$$를 생산하는 신경 네트워크 $$h_\phi$$
    * 콘텍스트 $$\pmb{\mathrm{c}}$$로 조건화된 새로운 샘플들을 생성하는 조건부 확산 모델
* [그림 3](#Fig3) 참고.

<div id="Fig3"></div>
![Fig3](/assets/fsdm/fig3.png)

그림 3. Few-Shot 확산 모델.

* **Generative Model**
    * 생성 모델은 아래와 같이 콘텍스트 넷 $$h_\phi$$로 생산된 콘텍스트 $$\pmb{\mathrm{c}}$$를 통해 셋 $$\mathrm{X}$$으로 조건화된 조건부 확산 모델임.
    <br/><br/>
    <div id="Eq5"></div>
    ![Eq5](/assets/fsdm/eq5.png)
    <br/><br/>
    * 실제로, 비전 트랜스포머(ViT)를 콘텍스트 넷 $$h_\phi$$로 사용하지만, UNet 인코더로도 실험함. [3.1절](#Sec3.1)에서 ViT의 사용에 관해 이야기를 함.
    * 생성 경로 $$p_\theta (\mathrm{x}_{t-1} \vert \mathrm{x}_{t}, \pmb{\mathrm{c}})$$는 UNet으로 파라미터화되는데, 이는 확산 모델의 일반적인 관행임.
    * 그러나, 추가적인 콘텍스트 $$\pmb{\mathrm{c}}$$가 있으므로, $$\mathrm{x}_{t-1}$$를 예측하기 위해 $$\mathrm{x}_{t}$$ 안의 정보와 $$\pmb{\mathrm{c}}$$를 결합하기 위해 UNet이 필요함.
    * 이 연구에서 이를 위해 두 메인 메커니즘을 고려함.
        * [FiLM]에 기반을 둔 메커니즘
        * [[86]]에서 영감을 얻어, Learnable Attentive Conditioning(LAC)을 제안함.
    * 이것들은 [3.2절](#Sec3.2)에서 더 자세히 이야기를 함.
    * Prior는 또한 $$\pmb{\mathrm{c}}$$로 조건화, 즉 $$p_\theta (\mathrm{x}_{T} \vert \pmb{\mathrm{c}})$$와 같이 될 수 있으나, 간단히 하기 위해 표준 무조건부 가우시안 $$p_\theta (\mathrm{x}_{T}) = \mathcal{N}(0, \mathrm{I})$$을 사용함.
<br/><br/>
* **Inference Model**
    * DDPM와 FSDM의 특수한 구조가 주어져 있을 때, 추론 모델은 파라미터가 없고 $$\pmb{\mathrm{c}}$$로 조건화할 필요가 없음. 이는 트레이닝 동안 상당한 간단화임.
    * 실제로 FSDM은 [Eq. 1](#Eq1)에서 나타난 것과 같이 노이즈를 추가하는 각 스텝에서 데이터 내의 정보를 열화시키는 파라미터 없는 확산 posterior를 도입함.
<br/><br/>
* **Loss and Training**
    * [Eq. 3](#Eq3)의 조건부 버전, 즉 $$L_\mathrm{FSDM} = L_0^{\pmb{\mathrm{c}}} + \sum_{t=2}^{T}L_{t-1}\pmb{\mathrm{c}} + L_{T}^{\pmb{\mathrm{c}}}$$으로 표현될 수 있음.
    * 보통 DDPM과 같이, loss는 레이어당 항들의 합으로 분리될 수 있고, 독립적으로 계산될 수 있음. 그래서 트레이닝은 Monte Carlo 샘플링 항에 의한 목적(objective)의 효율적인 확률적 (stochastic) 추정을 얻을 수 있다는 같은 이점을 활용함.
    * 따라서 레이어당 조건부 loss $$L_{t-1}^{\pmb{\mathrm{c}}}$$는 다음과 같이 공식화될 수 있음.
    <br/><br/>
    <div id="Eq6"></div>
    ![Eq6](/assets/fsdm/eq6.png)
    <br/><br/>
    * $$L_{T}^{\pmb{\mathrm{c}}}$$는 본 모델 공식화에서 무조건부이고 고정되어 있고, $$L_{0}^{\pmb{\mathrm{c}}}$$는 부정 (negated) 조건부 이산화 (discretized) 정규 likelihood, 즉 $$L_{0}^{\pmb{\mathrm{c}}} = \mathbb{E}_{q(\mathrm{x}_1 \vert \mathrm{x}_0)} [-\log p_\theta (\mathrm{x}_0 \vert \mathrm{x}_1, \pmb{\mathrm{c}})]$$임.
<br/><br/>
* 콘텍스트 넷은 $$\mathrm{x}$$에 있는 입력 샘플과 레이블을 공유하는 작은 셋 $$\mathrm{X}$$에서 정보를 추출함.
* 일반적으로 트레이닝 중 콘텍스트는 입력 종속적인 $$\mathrm{x} \in \mathrm{X}$$ 또는 입력 독립적인 $$\mathrm{x} \notin \mathrm{X}$$일 수 있음.
    * 입력 종속적인 시나리오에서, 셋 $$\mathrm{X}$$가 주어지면 각 새로운 입력 $$\mathrm{x}$$에 대해 다른 콘텍스트 결합을 학습할 수 있음.
    * 입력 독립적인 시나리오에서, 셋 $$\mathrm{X}$$가 주어지면 콘텍스트 결합은 모든 $$\mathrm{x}$$에 대해 동일함.
* 트레이닝 동안의 두 가지 상황 공식을 모두 탐구함. 입력 독립적인 콘텍스트가 분포 내에서는 약간 더 잘 작동하지만 분포 외 시나리오에서는 성능이 떨어져, 테스트 시간에 잘못된 정보를 악용하는 조건화 메커니즘을 초래한다는 것을 알게 됨.
* 입력 종속적 콘텍스트의 트레이닝은 우수한 성능의 분포 외를 제공하고, 분포 외 조건화 품질과 분포 내 샘플 다양성을 trade-off 함.
* 이러한 발견에 따라 결합 메커니즘이 병목 현상으로 작동하는, 입력 종속적 접근 방식으로 모델을 트레이닝함. 테스트 시간에 일반적인 분포 내 또는 분포 외 셋에서 조건부 및 few-shot 샘플링이 수행됨.

<div id="Sec3.1"></div>
### ViT as Set Encoder

<div id="Fig4"></div>
![Fig4](/assets/fsdm/fig4.png)

그림 4. sViT 구조. 입력은 이미지의 셋 $$\pmb{\mathrm{X}}$$임. 이것들은 겹치지 않는 패치들로 쪼개지고 패치 색으로 표시된 것과 같이, 공유된 위치 (positional) 인코딩을 사용하여 트랜스포머에 입력됨. sViT는 콘텍스트를 벡터($$\mathrm{V}$$)나 시각 토큰들($$\mathrm{T}$$)의 모음으로 출력함. DDPM은 FiLM이나 어텐션을 사용하여 이 정보로 조건화됨.

* 트랜스포머는 자연어 처리 task 및 텍스트 생성에서 사실상 (de-facto) 표준임. 최근 비전 트랜스포머(ViT)는 시각 task에 대한 어텐션의 일반적인 힘을 해제하였음. 그러나 생성을 위한 잠재 변수 모델에서의 트랜스포머의 사용은 여전히 제한적임.
* ViT는 패치 수준에서 이미지를 처리하는 유연한 방법을 제공함. [[57]]과 유사하게 이미지 셋을 다루는 ViT(sViT)를 채택함.
    * 특히 소량의 이미지 셋(1-10)을 처리하고자 함.
* 근본적인 아이디어는 이 셋에서 전역 정보를 추출하고자 하는 것이고 각 패치는 이미지 내 특정 영역에 대한 전역 정보를 포함해야 한다는 것임([그림 4](#Fig4)).
* 일반적으로 이 ViT 인코더를 $$\mathrm{ViT}(\pmb{\mathrm{X}}, t; \phi)$$를 사용하여 레이어 임베딩 $$t$$로 조건화할 수 있음.
    * 이렇게 하여 레이어 당 종속적인 (per layer-dependent) 콘텍스트를 학습하는 단순한 방법을 얻을 수 있는데, 큰 $$T$$에 대해서는 거칠고, 작은 $$t$$에 대해서는 더 정제됨.
* 토큰을 입력으로 사용하면 일반적인 도메인에 구애받지 않는 few-shot 잠재 변수 생성기에 대한 문이 열림.
    * 본 접근 방식은 입력 셋을 토큰화하고 패치 임베딩 레이어를 fine-tuning 하는 모든 모달리티(텍스트, 음성, 시각)로, 셋 인코더나, 조건화 메커니즘 또는 생성 프로세스를 수정할 필요 없이, few-shot 및 조건부 생성에 쉽게 사용될 수 있음.

<div id="Sec3.2"></div>
### Conditioning the Generative Process

<div id="Table1"></div>
![Table1](/assets/fsdm/table1.png)

표 1. 여러 조건화 메커니즘들.

* 패치를 처리하고 $$\pmb{\mathrm{c}}$$를 얻은 후, DDPM 생성 경로를 조건화하는 방법을 찾을 필요가 있음.
* 두 가지 다른 형태로 $$\pmb{\mathrm{c}}$$와 작업함. [표 1](#Table1)에 요약된 대로.
    * <i>벡터</i> $$\pmb{\mathrm{c}} \in \mathbb{R}^d$$
    * 또는 $$N$$개의 <i>토큰</i> 모음 $$\pmb{\mathrm{c}} \in \mathbb{R}^{N \times d}$$
* 이 연구에서 두 가지 메인 조건화 메커니즘을 고려함.
    * [FiLM]
    * [[86]]에서 영감을 얻은 Learnable Attentive Conditioning (LAC)
<br/><br/>
* **Vector ($$\mathrm{V}$$)**
    * 한 가지 접근 방식은 DDPM UNet 내 중간 특징점 (feature) 맵들 $$\pmb{\mathrm{u}}$$를 $$\pmb{\mathrm{c}}$$로 조건화하는 것이 될 수 있는데, 예를 들어 [FiLM] 같은 메커니즘을 사용하면, $$\pmb{\mathrm{u}} = m(\pmb{\mathrm{c}})\pmb{\mathrm{u}} + b(\pmb{\mathrm{c}})$$이 되고, 여기서 $$m$$과 $$b$$는 학습 가능하고 콘텍스트에 종속임.
    * 생성 모델의 특수한 구조가 주어지는데, 즉 조건화 메커니즘은 파라미터들 $$\theta$$를 공유하고 오직 스텝 $$t$$의 임베딩을 통해서만 달라져, 조건화 메커니즘은 일반적으로 $$\pmb{\mathrm{u}} = f(\pmb{\mathrm{u}}, \pmb{\mathrm{c}}, t) = f(\pmb{\mathrm{u}}_t, \pmb{\mathrm{c}}_t)$$와 같이 쓰일 수 있음.
    * $$\pmb{\mathrm{c}}$$를 스텝 임베딩과 병합하여 각 레이어를 조건화할 수 있고, 일반적인 스텝 당 조건화 메커니즘을 정의함.
    * 실제로 $$\pmb{\mathrm{u}}(\pmb{\mathrm{c}}, t) = m(\pmb{\mathrm{c}})\pmb{\mathrm{u}}(t) + b(\pmb{\mathrm{c}})$$가 가장 성능이 좋고 유연한 접근 방식임을 발견함.
<br/><br/>
* **Tokens ($$\mathrm{T}$$)**
    * 대안으로, $$\pmb{\mathrm{c}}$$는 변수들의 모음 $$\pmb{\mathrm{c}} = \{\pmb{\mathrm{c}}_{sp}\}_{s=1, p=1}^{N_s, N_p}$$가 될 수 있는데, 여기서 $$N_p$$는 샘플 당 패치의 수이고, $$N_s$$는 셋 내 샘플의 수임.
    * 이 경우, 어텐션은 콘텍스트 $$\pmb{\mathrm{c}}$$와 특징점 맵 $$\pmb{\mathrm{u}}$$ 사이의 정보를 결합하는 데 사용될 수 있음. 원리상 패치들은 직접 사용이 가능한데, 즉 $$\pmb{\mathrm{u}} = \mathrm{att}(\pmb{\mathrm{u}}, \{\pmb{\mathrm{c}}_{sp}\}_{s=1, p=1}^{N_s, N_p})$$. 그러나, 이 접근 방식은 셋 내 샘플 수 $$N_s$$로 나쁘게 스케일화함.
    * 또 다른 옵션은 셋 차원(dimension)의 평균 $$\pmb{\mathrm{c}}_p = \frac{1}{N_s}\sum_{s=1}^{N_s}\pmb{\mathrm{c}}_{sp}$$을 구하여 ViT에 입력시키는 $$N_p$$개의 토큰 $$\{\pmb{\mathrm{c}}_{p}\}_{p=1}^{N_p}$$을 구하는 패치 당 결합을 사용하는 것임. 그러면 패치 당 평균화된 토큰 $$\pmb{\mathrm{u}} = \mathrm{att}(\pmb{\mathrm{u}}, \{\pmb{\mathrm{c}}_{p}\}_{p=1}^{N_p})$$에 교차 어텐션(cross-attention)을 사용하여 DDPM을 조건화함.
    * 패치 당 결합을 사용하여, DDPM을 조건화하는 데 사용된 토큰의 수를 증가시키지 않고 샘플들의 임의의 개수를 처리할 수 있고, 더 중요한 것은, 콘텍스트 $$\pmb{\mathrm{c}}$$ 내 서로 다른 샘플들로부터 정보를 결합할 수 있다는 것임.

<div id="Sec3.3"></div>
### Variational FSDM

* FSDM의 공식화에 대한 대안으로, 콘텍스트 $$\pmb{\mathrm{c}}$$가 잠재 변수이고 셋 $$\pmb{\mathrm{X}}$$이 $$\pmb{\mathrm{c}}$$로 조건화되어 생성되는 잠재 변수 모델을 특정화할 수도 있음.
* 이 모델을 Variational FSDM(VFSDM)이라고 칭하며 다음과 같이 씀.
<br/><br/>
<div id="Eq7"></div>
![Eq7](/assets/fsdm/eq7.png)
<br/><br/>
* 이 경우, 추론 모델은 파라미터가 없는 확산 posterior와 $$\pmb{\mathrm{c}}$$에 대한 파라미터화된 인코더의 조합이 될 것임.
<br/><br/>
<div id="Eq8"></div>
![Eq8](/assets/fsdm/eq8.png)
<br/><br/>
* 게다가, negative ELBO는 인코더 $$q_\phi (\pmb{\mathrm{c}} \vert \pmb{\mathrm{X}}_0)$$과 prior $$p(\pmb{\mathrm{c}})$$ 사이의 추가적인 KL 항을 포함할 것임.
<br/><br/>
<div id="Eq9"></div>
![Eq9](/assets/fsdm/eq9.png)
<br/><br/>
* 원래 이 모델로 연구하였으나, 트레이닝이 더 어려워져, 성능이 떨어지거나 저조한 조건화 속성을 초래함을 발견하였음.

<div id="Sec6"></div>
## Conclusion

* 비전 트랜스포머와 확산 모델의 진보를 활용하여, 테스트 시간에 다양한 생성 프로세스에 빠르게 적응할 수 있는 유연한 프레임워크인 Few-Shot 확산 모델을 제시함.
* 풍부하고 표현력 있는 정보로 확산 모델을 조절하는 것이 분포 내 및 분포 외의 광범위한 실험에서 어떻게 우수한 성능을 제공하는지 보여줌.
* Few-shot 생성은 현실적이고 복잡한 이미지셋에서 수행되어, 대규모 few-shot 잠재 변수 생성 모델의 유망한 방향을 보여줌.

[Few-Shot Diffusion Models]: https://arxiv.org/abs/2205.15463
[Github]: https://github.com/georgosgeorgos/few-shot-diffusion-models
[FiLM]: https://arxiv.org/abs/1709.07871
[57]: https://arxiv.org/abs/2112.13492
[86]: https://arxiv.org/abs/2112.10752